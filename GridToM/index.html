<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>From Black Boxes to Transparent Minds: Evaluating and Enhancing the Theory of Mind in Multimodal Large Language Models</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">From Black Boxes to Transparent Minds: Evaluating and Enhancing the
              Theory of Mind in Multimodal Large Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block"><a href="https://annaisavailable.github.io/index.html" target="_blank">Xinyang
                  Li</a><sup>*</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=8hceDf0AAAAJ"
                  target="_blank">Siqi Liu</a><sup>*</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=Cb29A3cAAAAJ"
                  target="_blank">Bochao Zou</a><sup>â€ </sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=A1gA9XIAAAAJ"
                  target="_blank">Jiansheng Chen</a>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=32hwVLEAAAAJ"
                  target="_blank">Huimin Ma</a><sup>â€ </sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">University of Science and Technology Beijing<br>Forty-Second International
                Conference on Machine Learning 2025</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal
                  Contribution</small>,<small><sup>â€ </sup>Indicates Corresponding Author</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/annaisavailable/GridToM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- HuggingFace Link -->
                <span class="link-block">
                  <a href="https://huggingface.co/<Hugging Face ID>" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i></i>ðŸ¤—
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="static/images/introduction.png" alt="MY ALT TEXT" />
        <h2 class="content has-text-centered">
          This illustration highlights the integration of different levels of ToM: recognizing an agentâ€™s desire (Cooper
          wants to pilot), a first-order belief (he believes he can do it), and a second-order belief (he believes TARS
          perceives it as risky). These nested mental states are crucial in evaluating advanced ToM.
        </h2>
      </div>
    </div>
  </section>

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              As large language models evolve, there is growing anticipation that they will emulate human-like Theory of
              Mind (ToM) to assist with routine tasks.
              However, existing methods for evaluating machine ToM focus primarily on unimodal models and largely treat
              these models as black boxes, lacking an interpretative exploration of their internal mechanisms. In
              response, this study adopts an approach based on internal mechanisms to provide an interpretability-driven
              assessment of ToM in multimodal large language models (MLLMs).
              Specifically, we first construct a multimodal ToM test dataset, GridToM, which incorporates diverse belief
              testing tasks and perceptual information from multiple perspectives. Next, our analysis shows that
              attention heads in multimodal large models can distinguish cognitive information across perspectives,
              providing evidence of ToM capabilities. Furthermore, we present a lightweight, training-free approach that
              significantly enhances the model's exhibited ToM by adjusting in the direction of the attention head.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <!-- Dataset -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title">GridToM</h2>
          <div class="content has-text-justified">
            <p>
              For all agents involved in the event, we provide full physical perspective information across the
              timeline. When an agent closes a door, we mask its perception of any information beyond the door to
              simulate realistic sensory limitations. Each sample contains three types of questions (illustrated on the
              right). For each video-text pair, the accompanying text annotations include environment descriptions,
              initial belief assessments, first-order belief assessments, and second-order belief assessments.
            </p>
          </div>
          <img src="static/images/gridtom.png" alt="MY ALT TEXT" />
          <h2 class="content has-text-centered">
            A FB sample in GridToM includes an omniscient-perspective video covering the entire event timeline, along
            with omniscient-perspective textual descriptions for each time interval.
          </h2>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container" style="width: 750px; margin: auto;">
        <h2 class="title">
          Video Examples
        </h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <img src="static/images/tb-all.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered" style="font-size: 1rem;">
              The video frame sequences of TB test from three distinct perspectives.
            </h2>
          </div>
          <div class="item">
            <img src="static/images/fb-all.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered" style="font-size: 1rem;">
              The video frame sequences of FB test from three distinct perspectives.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container" style="width: 750px; margin: auto;">
        <h2 class="title">
          Text Examples
        </h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <img src="static/images/initial-belief.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered" style="font-size: 1rem;">
              The textual annotations for the initial belief task.
            </h2>
          </div>
          <div class="item">
            <img src="static/images/frbelief.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered" style="font-size: 1rem;">
              The textual annotations for the first order belief task in the TB and FB tests.
            </h2>
          </div>
          <div class="item">
            <img src="static/images/sebelief_tb.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered" style="font-size: 1rem;">
              The textual annotations for the second order belief task in the TB test.
            </h2>
          </div>
          <div class="item">
            <img src="static/images/sebelief_fb.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered" style="font-size: 1rem;">
              The textual annotations for the second order belief task in the FB test.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Dataset -->

  <!-- Method -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title">Method Overview</h2>
          <img src="static/images/overview.png" alt="MY ALT TEXT" />
          <h2 class="content has-text-centered">
            Overview of Our Workflow. We first constructed the GridToM dataset and conducted benchmark testing of MMLMs
            on it. Subsequently, we input video-text pairs to probe the internal attention representations of the
            models. Using logistic regression, we performed binary classification on the representations of positive and
            negative samples to identify attention heads that are sensitive to perspective separation and belief
            representation. Targeted interventions were then applied to the top K most sensitive attention heads during
            inference.
          </h2>
        </div>
      </div>
    </div>
  </section>
  <!-- End Method -->

  <!-- Results -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title">Quantitative Results</h2>
          <img src="static/images/init-results.png" style="width:30%; display:block; margin:0 auto;" alt="MY ALT TEXT" />
          <img src="static/images/results.png" alt="MY ALT TEXT" />
        </div>
      </div>
    </div>
  </section>
  <!-- End Results -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>